<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>GMM_reading</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h2 id="gaussian-mixture-models">Gaussian Mixture Models</h2>
<h2 id="use-cases-of-gmm1">Use cases of GMM:1</h2>
<ul>
<li><strong>Recommender systems</strong> that make recommendations to
users based on preferences (such as Netflix viewing patterns) of similar
users (such as neighbors).</li>
<li><strong>Anomaly detection</strong> that identifies rare items,
events or observations which deviate significantly from the majority of
the data and do not conform to a well defined notion of normal
behavior.</li>
<li><strong>Customer segmentation</strong> that aims at separating
customers into multiple clusters, and devise targeted marketing strategy
based on each cluster’s characteristics.</li>
</ul>
<h2 id="when-is-gmm-better-than-k-means">When is GMM better than
K-Means?</h2>
<p>Imagine you are a Data Scientist who builds a recommender for selling
cars using K-Means clustering and you have two clusters. Everybody in
cluster A is recommended to buy car A which costs <strong>100k</strong>
with a <strong>25k</strong> profit margin and everyone in cluster B is
recommended to buy car B which costs <strong>50k</strong> with a
<strong>10k</strong> profit margin.</p>
<p>Let’s say you want to get as many people in cluster A as possible,
why not use an algorithm that informs you of exactly how likely somebody
would be interested in purchasing car A, instead of one that only tells
you a hard yes or no (This is what K-Means does!).</p>
<p>With GMM, not only will you be getting the predicted cluster labels,
the algorithm will also give you the probability of a data point
belonging to a cluster. How amazing is that!</p>
<p>Whoever is selling those cars should definitely work on a better plan
for a customer with a 90% chance of purchasing than for someone with a
75% chance of purchasing, even though they might show up in the same
cluster.</p>
<p><img src="https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%201/images/car.png" style="width: 60%"></p>
<h2 id="what-are-gaussian-mixture-models-gmm">What are Gaussian Mixture
Models (GMM)?</h2>
<p>Put simply, Gaussian Mixture Models (GMM) is a clustering algorithm
that:</p>
<ul>
<li>Fits Gaussian distributions to your data</li>
<li>The data scientist (you) needs to determine the number of gaussian
distributions (<code>k</code>)</li>
</ul>
<p style="color: blue">
Hard vs Soft Clustering:
</p>
<ul>
<li><strong>Hard clustering</strong> algorithms cluster each data point
in exactly one cluster.</li>
<li><strong>Soft clustering</strong> algorithms can cluster data in
partially one cluster and partially others.</li>
</ul>
<p><em>GMM is a soft clustering algorithm.</em></p>
<h3 id="background">Background:</h3>
<p>A Gaussian mixture is a weighted combination of (<code>k</code>)
Gaussians, where each is identified by the following parameters:</p>
<ol type="1">
<li>a mean vector <span
class="math inline">\(\boldsymbol{\mu}_i\)</span></li>
<li>a covariance matrix <span
class="math inline">\(\boldsymbol{\Sigma}_i​\)</span></li>
<li>a component weight <span class="math inline">\(\pi_i\)</span> that
indicates the contribution of the <span
class="math inline">\(i\)</span>th Gaussian</li>
</ol>
<p>When put altogether, the pdf of the mixture model is formulated
as:</p>
<p><span class="math display">\[
p(\boldsymbol{x}) = \sum_{i=1}^K\pi_i
\mathcal{N}(x|\boldsymbol{\mu_i,\Sigma_i}), \\ \sum_{i=1}^K\pi_i =1
\]</span></p>
<h2 id="example-1-1-dimensional-gaussian-mixture">Example 1:
1-Dimensional Gaussian Mixture:</h2>
<p>Let’s look at a mixture of 3 univariate Gaussians with</p>
<ul>
<li>means equal to <strong>2, 5, 8</strong> respectively</li>
<li>std equal to <strong>0.2, 0.5, 0.8</strong> respectively</li>
<li>component weight equal to <strong>0.3, 0.3, 0.4</strong>
respectively</li>
</ul>
<p><img src="https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%201/images/onedim.png" width="550" height="350"></p>
<p>Thus, the means determine the centers of the mixed Gaussians; the
standard deviations determine the width and shape of the mixed
Gaussians; the weights determine the contributions of the Gaussians to
the mixture.</p>
<p>Let’s fit a GMM with <code>n_components=3</code> to our simulated
data and plot the prior probabilities. The
<strong>GaussianMixture</strong> class from
<strong>Scikit-learn</strong> allows us to estimate the parameters of a
Gaussian mixture distribution.</p>
<p><strong>GaussianMixture.predict_proba_</strong> evaluates the
components’ density for each sample or for sample <span
class="math inline">\(x_n\)</span> the probability <span
class="math inline">\(p(i|x_{n})\)</span>.</p>
<p><img src="https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%201/images/onedim_prior.png"></p>
<p>To interpret the predicted probabilities, let’s take a look at the
point colored in black as an example. On the Gaussian mixture pdf, the
point is at the the peak of the first bell-shaped curve. Its
corresponding probability of belonging to cluster 1 is equal to 1, which
demonstrates that the probability of the center of a Gaussian
distribution belonging to its own cluster is 100%.</p>
<details>
<summary>
Click here for the code used for this example!
</summary>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.stats <span class="im">as</span> ss</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.mixture <span class="im">import</span> GaussianMixture</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>means<span class="op">=</span>[<span class="dv">2</span>,<span class="dv">5</span>,<span class="dv">8</span>]</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>stds<span class="op">=</span>[<span class="fl">0.2</span>, <span class="fl">0.5</span>, <span class="fl">0.8</span>]</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>weights<span class="op">=</span>[<span class="fl">0.3</span>, <span class="fl">0.3</span>, <span class="fl">0.4</span>]</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>mixture_idx <span class="op">=</span> np.random.choice(<span class="dv">3</span>, size<span class="op">=</span><span class="dv">10000</span>, replace<span class="op">=</span><span class="va">True</span>, p<span class="op">=</span>weights)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># generate 10000 possible values of the mixture</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.fromiter((ss.norm.rvs(loc<span class="op">=</span>means[i], scale<span class="op">=</span>stds[i]) <span class="cf">for</span> i <span class="kw">in</span> mixture_idx), dtype<span class="op">=</span>np.float64)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co"># create x axis of the plot </span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>xs <span class="op">=</span> np.linspace(X.<span class="bu">min</span>(), X.<span class="bu">max</span>(), <span class="dv">300</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>ps <span class="op">=</span> np.zeros_like(xs)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> mu, s, w <span class="kw">in</span> <span class="bu">zip</span>(means, stds, weights):</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    ps <span class="op">+=</span> ss.norm.pdf(xs, loc<span class="op">=</span>mu, scale<span class="op">=</span>s) <span class="op">*</span> w</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co"># sort X in ascending order for plotting purpose</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>X_sorted <span class="op">=</span> np.sort(X.reshape(<span class="op">-</span><span class="dv">1</span>)).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="co"># fit the GMM</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>GMM <span class="op">=</span> GaussianMixture(n_components<span class="op">=</span><span class="dv">3</span>, random_state<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>GMM.fit(X_sorted)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="co"># store the predicted probabilities in y1_prob</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> GMM.predict_proba(X_sorted)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the Gaussian mixture pdf</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">6</span>))</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>plt.plot(xs, ps, label<span class="op">=</span><span class="st">&#39;pdf of the Gaussian mixture&#39;</span>)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&quot;X&quot;</span>, fontsize<span class="op">=</span><span class="dv">15</span>)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&quot;P&quot;</span>, fontsize<span class="op">=</span><span class="dv">15</span>)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&quot;Univariate Gaussian mixture&quot;</span>, fontsize<span class="op">=</span><span class="dv">15</span>)</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the predicted prior probabilities</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>plt.plot(X_sorted, probs[:,<span class="dv">0</span>], label<span class="op">=</span><span class="st">&#39;Predicted Prob of x belonging to cluster 1&#39;</span>)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>plt.plot(X_sorted, probs[:,<span class="dv">1</span>], label<span class="op">=</span><span class="st">&#39;Predicted Prob of x belonging to cluster 2&#39;</span>)</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>plt.plot(X_sorted, probs[:,<span class="dv">2</span>], label<span class="op">=</span><span class="st">&#39;Predicted Prob of x belonging to cluster 3&#39;</span>)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>plt.scatter(<span class="dv">2</span>, <span class="fl">0.6</span>, color<span class="op">=</span><span class="st">&#39;black&#39;</span>)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>plt.scatter(<span class="dv">2</span>, <span class="fl">1.0</span>, color<span class="op">=</span><span class="st">&#39;black&#39;</span>)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>plt.plot([<span class="dv">2</span>, <span class="dv">2</span>], [<span class="fl">0.6</span>, <span class="fl">1.0</span>],<span class="st">&#39;--&#39;</span>, color<span class="op">=</span><span class="st">&#39;black&#39;</span>)</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>plt.legend(bbox_to_anchor<span class="op">=</span>(<span class="fl">0.6</span>,<span class="fl">0.7</span>), borderaxespad<span class="op">=</span><span class="dv">0</span>)</span></code></pre></div>
</details>
<h2 id="example-2-2-dimensional-gaussian-mixture">Example 2:
2-Dimensional Gaussian Mixture:</h2>
<p>In this example, you have a simulated 2-dimensional data that looks
like this:</p>
<p><img src="https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%201/images/twodim.png" width="550" height="350"></p>
<details>
<summary>
Click here for the code used for simulating the 2d dataset!
</summary>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>means <span class="op">=</span> [(<span class="dv">1</span>,<span class="dv">5</span>), (<span class="dv">2</span>,<span class="dv">1</span>), (<span class="dv">6</span>,<span class="dv">2</span>)]</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>cov1 <span class="op">=</span> np.array([[<span class="fl">0.5</span>, <span class="fl">1.0</span>],[<span class="fl">1.0</span>, <span class="fl">0.8</span>]])</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>cov2 <span class="op">=</span> np.array([[<span class="fl">0.8</span>, <span class="fl">0.4</span>],[<span class="fl">0.4</span>, <span class="fl">1.2</span>]])</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>cov3 <span class="op">=</span> np.array([[<span class="fl">1.2</span>, <span class="fl">1.3</span>],[<span class="fl">1.3</span>, <span class="fl">0.9</span>]])</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>covs <span class="op">=</span> [cov1, cov2, cov3]</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> [<span class="fl">0.3</span>, <span class="fl">0.3</span>, <span class="fl">0.4</span>]</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>mixture_idx <span class="op">=</span> np.random.choice(<span class="dv">3</span>, size<span class="op">=</span><span class="dv">10000</span>, replace<span class="op">=</span><span class="va">True</span>, p<span class="op">=</span>weights)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co"># generate 10000 possible values of the mixture</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.fromiter(chain.from_iterable(multivariate_normal.rvs(mean<span class="op">=</span>means[i], cov<span class="op">=</span>covs[i]) <span class="cf">for</span> i <span class="kw">in</span> mixture_idx), </span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>            dtype<span class="op">=</span><span class="bu">float</span>)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>X.shape <span class="op">=</span> <span class="dv">10000</span>, <span class="dv">2</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>xs1 <span class="op">=</span> y[:,<span class="dv">0</span>] </span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>xs2 <span class="op">=</span> y[:,<span class="dv">1</span>]</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>plt.scatter(xs1, xs2, label<span class="op">=</span><span class="st">&quot;data&quot;</span>)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> <span class="bu">len</span>(means)</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> l, pair <span class="kw">in</span> <span class="bu">enumerate</span>(means):</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    plt.scatter(pair[<span class="dv">0</span>], pair[<span class="dv">1</span>], color<span class="op">=</span><span class="st">&#39;red&#39;</span>)</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> l <span class="op">==</span> L<span class="op">-</span><span class="dv">1</span>:</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>plt.scatter(pair[<span class="dv">0</span>], pair[<span class="dv">1</span>], color<span class="op">=</span><span class="st">&#39;red&#39;</span>, label<span class="op">=</span><span class="st">&quot;mean&quot;</span>)</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&quot;$x_1$&quot;</span>)</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&quot;$x_2$&quot;</span>)</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&quot;Scatter plot of the bivariate Gaussian mixture&quot;</span>)</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>plt.legend()</span></code></pre></div>
</details>
<p>Like before, to work with GMM, we can use the
<strong>GaussianMixture</strong> function from
<strong>sklearn.mixture</strong>. We fit a GMM with <strong>n_components
= 3</strong> to the simulated dataset, and plot the clustering result as
follows:</p>
<p><img src="https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%201/images/twodim_pred.png" width="550" height="350"></p>
<p>Awesome! The fitted clusters indeed match the individual Gaussians we
simulated, and the ellipses drawn based on the estimated parameter
values (means, covariances, weights) contain the clusters.</p>
<p>The default value of <strong>covariance_type</strong> in GMM is
<code>full</code>, which allows each component (Gaussian) to have its
own covariance matrix.</p>
<p>Since our dataset was simulated using three different covariance
matrices, using the default covariance_type value would work the
best.</p>
<p>However, note that sometimes you can’t use <strong>covariance_type =
full</strong>, because you won’t be able to invert it and that will give
you an error.</p>
<h2 id="example-3-image-segmentation">Example 3: Image Segmentation</h2>
<p>Image segmentation is the process of segmenting an image into
multiple important regions.</p>
<p>We can use a GMM to segment an image into <strong>K</strong> regions
<code>(n_components = K)</code> according to significant colors.</p>
<p>Each pixel would be a data point with three features (r, g, b) (Or 1
feature if greyscale).</p>
<p>For instance, if we are working with a 256 <span
class="math inline">\(\times\)</span> 256 image, you would have 65536
pixels in total and your data <span class="math inline">\(X\)</span>
would have a shape of 65536 <span class="math inline">\(\times\)</span>
3.</p>
<p>Let’s look at an example using a picture of a house cat:</p>
<p><img src="https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%201/images/gauss-cat.jpeg" style="width: 50%"></p>
<p>First let’s segment our image using 2 gaussian distributions;</p>
<p>Then we replace each pixel with the “average color” or the mean RGB
values of the gaussian distribution it belongs to:</p>
<p><img src="https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%201/images/gauss-cat-2.jpeg" style="width: 50%"></p>
<p>Similarly, if we increase the number of components to 8:</p>
<p><img src="https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%201/images/gauss-cat-8.jpeg" style="width: 50%"></p>
<p>Our segmented image looks remarkably similar to the original, even
though it uses only 8 colors!</p>
<details>
<summary>
Click here for the code used for this example!
</summary>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> cv2</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.mixture <span class="im">import</span> GaussianMixture <span class="im">as</span> GMM</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> cv2.imread(<span class="st">&#39;cat.jpeg&#39;</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># If img is greyscale, then change to .reshape(-1, 1):</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> img.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">3</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co"># The number of components; you can change this to a positive integer of your choice!:</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>gmm <span class="op">=</span> GMM(n_components<span class="op">=</span>n, covariance_type<span class="op">=</span><span class="st">&#39;tied&#39;</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>gmm.fit(x)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> gmm.predict(x)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>seg <span class="op">=</span> np.zeros(x.shape)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> label <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    seg[labels <span class="op">==</span> label] <span class="op">=</span> gmm.means_[label]</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>seg <span class="op">=</span> seg.reshape(img.shape).astype(np.uint8)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>cv2.imwrite(<span class="ss">f&#39;gauss-cat-</span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">.jpeg&#39;</span>, seg)</span></code></pre></div>
</details>
</body>
</html>
